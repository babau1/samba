Classification on pima for pima with adaboost.

Database configuration : 
	- Database name : pima
	- View name : pima	 View shape : (768, 8)
	- Learning Rate : 0.75
	- Labels used : No, Yes
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 82, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8107638888888888
		- Score on test : 0.796875

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8107638888888888
		- Score on test : 0.796875

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.7715621890547264
		- Score on test : 0.7608955223880597

Test set confusion matrix : 

╒═════╤══════╤═══════╕
│     │   No │   Yes │
╞═════╪══════╪═══════╡
│ No  │  110 │    15 │
├─────┼──────┼───────┤
│ Yes │   24 │    43 │
╘═════╧══════╧═══════╛



 Classification took 0:00:27

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.2864185876279779
- Feature index : 6, feature importance : 0.2611699740901631
- Feature index : 1, feature importance : 0.19150350229318688
- Feature index : 5, feature importance : 0.14219125175175978
- Feature index : 2, feature importance : 0.033797683851666704
- Feature index : 0, feature importance : 0.03267007823577069
- Feature index : 4, feature importance : 0.02898192653124848
- Feature index : 3, feature importance : 0.023266995618226316


 Estimator error | Estimator weight
0.2604166666666666 | 0.09926600218492043
0.35305164319248816 | 0.05759765475027231
0.3930933089892846 | 0.04130469576816591
0.3738625060781595 | 0.04904146443705998
0.3656761715934983 | 0.05238229193591976
0.4050708664863199 | 0.03655465034837175
0.4344720415459155 | 0.025071095756555105
0.4380115322439037 | 0.02370240873753586
0.4721038787642747 | 0.010622741064157426
0.47365281807338405 | 0.01003178387247651
0.47504646374496945 | 0.009500240550041833
0.45508091119868327 | 0.01713345589892921
0.459248762054275 | 0.015536269733235585
0.4556193653633793 | 0.016926980902821243
0.4149514945945854 | 0.032670078235770686
0.48352492816957965 | 0.0062694018548841
0.45303785308502 | 0.017917255147029652
0.45691940592736946 | 0.01642863189900713
0.46418540788356877 | 0.013647275572065121
0.4660701881965122 | 0.012926797954468173
0.48377542739014856 | 0.006174009722402895
0.48436747083686066 | 0.005948567035391631
0.4697993829114915 | 0.011502343818662766
0.4740302917161194 | 0.009887798452993675
0.488595425294031 | 0.00433906314909366
0.4854221931648145 | 0.005546983043766212
0.48591666091604757 | 0.005358732406445037
0.4825566361699068 | 0.006638165373883606
0.44740701749273226 | 0.02008069624612954
0.4391388735575742 | 0.023266995618226313
0.45597285528088377 | 0.016791453740017545
0.47339144136960026 | 0.010131491421022713
0.4820675607089935 | 0.0068244432549754465
0.48294295344858407 | 0.006491034796786918
0.48603091208471283 | 0.005315236929735318
0.48649050119728754 | 0.0051402766941586985
0.48372246575971883 | 0.006194177679869134
0.4781956581153646 | 0.008299655311317124
0.46780858782776147 | 0.012262604612158653
0.4868132734180585 | 0.005017406275841894
0.47661746487189294 | 0.00890123028511894
0.46952279006846 | 0.011607948806208636
0.48741834065396206 | 0.0047870851865595004
0.48780603645549725 | 0.004639514761020427
0.4899927740455076 | 0.0038072662587503867
0.49018913316790413 | 0.003732541720854784
0.490377934769163 | 0.0036606942871539264
0.49055960692510575 | 0.0035915609118966647
0.485417693204356 | 0.005548696289602415
0.48607377250419537 | 0.005298920090330538
0.48812480992040524 | 0.004518182750986402
0.4884785349092291 | 0.004383551741108752
0.4907031549289914 | 0.003536935942609572
0.49087286210293285 | 0.003472357258213592
0.48416902627632014 | 0.006024130331350305
0.47315824537867934 | 0.010220453548337578
0.4803299302521411 | 0.007486376066737368
0.49163778252758 | 0.0031812917472395733
0.48878357587770793 | 0.004267453852298423
0.48910731521452855 | 0.004144242883366882
0.4864868713044978 | 0.0051416585177086966
0.4667197723998089 | 0.012678574224864491
0.46893899919097304 | 0.011830867594139097
0.4869246446872465 | 0.004975011304110754
0.49175543357922313 | 0.0031365248341595304
0.4918891740655117 | 0.0030856362188503754
0.4920186448305137 | 0.003036372664862442
0.49214404713060445 | 0.002988657547928877
0.48940327723444593 | 0.004031606644287258
0.4897005517953089 | 0.003918473728725066
0.4922575864838281 | 0.0029454565635455986
0.46685515802940225 | 0.012626845164569156
0.4745062584312897 | 0.009706259465140433
0.48155685809640414 | 0.007018972406937693
0.47786675432365244 | 0.008425012620472296
0.4914477958371897 | 0.0032535835400678875
0.479547041529712 | 0.0077846679159259075
0.47575357345091424 | 0.009230602635459823
0.4874142968622016 | 0.00478862442356185
0.46991341870779785 | 0.01145880625878775
0.4717549239006788 | 0.01075590352240137
0.4878333058547649 | 0.004629135297182294