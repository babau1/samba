Classification on pima for pima with adaboost.

Database configuration : 
	- Database name : pima
	- View name : pima	 View shape : (768, 8)
	- Learning Rate : 0.75
	- Labels used : No, Yes
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 39, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8402777777777778
		- Score on test : 0.8020833333333334

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8402777777777778
		- Score on test : 0.8020833333333334

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8173134328358209
		- Score on test : 0.7718208955223881

Test set confusion matrix : 

╒═════╤══════╤═══════╕
│     │   No │   Yes │
╞═════╪══════╪═══════╡
│ No  │  109 │    16 │
├─────┼──────┼───────┤
│ Yes │   22 │    45 │
╘═════╧══════╧═══════╛



 Classification took 0:00:26

 Classifier Interpretation : 
Feature importances : 
- Feature index : 1, feature importance : 0.2603929465423088
- Feature index : 5, feature importance : 0.24761094355468805
- Feature index : 7, feature importance : 0.14473295898675093
- Feature index : 6, feature importance : 0.09483834978305679
- Feature index : 2, feature importance : 0.07331402575271505
- Feature index : 3, feature importance : 0.07193075406312334
- Feature index : 4, feature importance : 0.054294547964658275
- Feature index : 0, feature importance : 0.052885473352699025


 Estimator error | Estimator weight
0.23958333333333326 | 0.10209091665758625
0.3323903116934684 | 0.061644774676348504
0.420552048361355 | 0.02833064868239668
0.3595924263087814 | 0.05101460508298998
0.38632612249830484 | 0.040906697765442694
0.4332404647655032 | 0.023746107009629897
0.42782364383321175 | 0.02569906351679121
0.4374083015991376 | 0.022247390101988857
0.46381451085300557 | 0.01281659245332986
0.4158637190493079 | 0.030033904519468205
0.40995892047715865 | 0.03218702654438346
0.394358372543178 | 0.03792313244181991
0.4156418477341906 | 0.030114644296050992
0.4378004101200236 | 0.022106558242203957
0.43326482377184417 | 0.023737338070830602
0.42867871094781135 | 0.025390379124225083
0.4247223822603735 | 0.02681995875789307
0.4240287484913786 | 0.027070950197778974
0.4526130304984719 | 0.01680515424799901
0.44055064937362587 | 0.021119540951019717
0.4512910027259059 | 0.01727694359034693
0.4468272018859565 | 0.018871785080919013
0.4496988080640526 | 0.017845473263503008
0.4267602699704518 | 0.02608316533032202
0.43791324556622313 | 0.022066036819855406
0.4326077665353713 | 0.023973911311331156
0.41395930028197925 | 0.030727337056444357
0.45388549702889963 | 0.0163512770930115
0.42460518694061655 | 0.026862358335244908
0.48074506976055376 | 0.006811377674607449
0.4763249280448345 | 0.00837711307646288
0.477663246808239 | 0.007902916483227112
0.4599820122214285 | 0.014179579314491958
0.4584520674800447 | 0.014724148793338686
0.44270588999927807 | 0.02034697708620533
0.4374572925130206 | 0.02222979272309858
0.4638252296318956 | 0.012812782642238944
0.4600009583969984 | 0.01417283730511625
0.45324753888494096 | 0.016578803680057774