Classification on pima for pima with adaboost.

Database configuration : 
	- Database name : pima
	- View name : pima	 View shape : (768, 8)
	- Learning Rate : 0.75
	- Labels used : No, Yes
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 45, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.9340277777777778
		- Score on test : 0.7604166666666666

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.9340277777777778
		- Score on test : 0.7604166666666666

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.9193233830845771
		- Score on test : 0.7363582089552239

Test set confusion matrix : 

╒═════╤══════╤═══════╕
│     │   No │   Yes │
╞═════╪══════╪═══════╡
│ No  │  102 │    23 │
├─────┼──────┼───────┤
│ Yes │   23 │    44 │
╘═════╧══════╧═══════╛



 Classification took 0:00:22

 Classifier Interpretation : 
Feature importances : 
- Feature index : 6, feature importance : 0.20957516545419833
- Feature index : 5, feature importance : 0.20125799702022446
- Feature index : 1, feature importance : 0.19282054437477886
- Feature index : 7, feature importance : 0.14014668121022175
- Feature index : 0, feature importance : 0.07853253642179608
- Feature index : 3, feature importance : 0.06860936717771501
- Feature index : 4, feature importance : 0.0638130008779089
- Feature index : 2, feature importance : 0.045244707463156286


 Estimator error | Estimator weight
0.2326388888888888 | 0.06447546750035169
0.27672722361045454 | 0.051903332105925674
0.32587422951704675 | 0.03926998470377744
0.3736567781765926 | 0.027906439690448256
0.3991931806762586 | 0.022086331371927732
0.3425448278623139 | 0.03522193998903449
0.34108308052216435 | 0.035572948230182866
0.43237598695238716 | 0.014703244017533054
0.4260857452449222 | 0.016090335791804265
0.37385028703707973 | 0.02786177614130568
0.4221070230941368 | 0.016970400827402636
0.4539784977374107 | 0.009973204098947886
0.45815446876167243 | 0.009063774497981965
0.44975478345410747 | 0.010894467217961288
0.4559152883847196 | 0.009551248567148765
0.405227574095189 | 0.02073045124811179
0.39654355031220145 | 0.022683832385320405
0.3944570275586484 | 0.023155313134545014
0.3836349870881518 | 0.025615137933344124
0.4540485441662494 | 0.009957938343951813
0.40016379343087 | 0.021867789608277226
0.36659793966762255 | 0.029542209733052282
0.38189012313285736 | 0.026014129209058604
0.3967239364076231 | 0.02264311167455984
0.3724520160337341 | 0.028184720038530554
0.43101184683685495 | 0.015003633003920646
0.3928317268134458 | 0.02352317579938613
0.3513899550120576 | 0.033112917655324384
0.3359171215263654 | 0.036819331630467224
0.43586901804053324 | 0.013935082376239
0.43471502952527497 | 0.014188700349722317
0.4554787006884928 | 0.00964633916714031
0.4502000783958762 | 0.010797268729069843
0.46749000639645155 | 0.007035143659668329
0.4754375164412116 | 0.005312084232917397
0.4249634481247623 | 0.016338360477330446
0.38621757983753696 | 0.025025839342340407
0.3953453445719122 | 0.022954479547601506
0.40938094861174656 | 0.019800979373177454
0.36773010031120434 | 0.02927897693630951
0.4366403576405859 | 0.013765646397923725
0.4024595218037694 | 0.021351585079700017
0.3898889294879337 | 0.024190609045979524
0.4351192588817028 | 0.014099843240530258
0.4001074281291982 | 0.02188047589476617