Classification on pima for pima with adaboost.

Database configuration : 
	- Database name : pima
	- View name : pima	 View shape : (768, 8)
	- Learning Rate : 0.75
	- Labels used : No, Yes
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 47, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8697916666666666
		- Score on test : 0.7447916666666666

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8697916666666666
		- Score on test : 0.7447916666666666

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8584477611940299
		- Score on test : 0.707044776119403

Test set confusion matrix : 

╒═════╤══════╤═══════╕
│     │   No │   Yes │
╞═════╪══════╪═══════╡
│ No  │  104 │    21 │
├─────┼──────┼───────┤
│ Yes │   28 │    39 │
╘═════╧══════╧═══════╛



 Classification took 0:00:23

 Classifier Interpretation : 
Feature importances : 
- Feature index : 1, feature importance : 0.3109932695908483
- Feature index : 5, feature importance : 0.21432901072437052
- Feature index : 6, feature importance : 0.17015714877275256
- Feature index : 7, feature importance : 0.12178398385988157
- Feature index : 3, feature importance : 0.09904582532410094
- Feature index : 2, feature importance : 0.045697474300278144
- Feature index : 0, feature importance : 0.02170884543541257
- Feature index : 4, feature importance : 0.016284441992355455


 Estimator error | Estimator weight
0.2309027777777777 | 0.08549286275069153
0.3512534157063087 | 0.043593664714742776
0.389588415961927 | 0.03190598440170976
0.38140890615187545 | 0.03435944247549549
0.37813694513518303 | 0.03534644902560933
0.39182151015697586 | 0.031239460128092023
0.38944101330491065 | 0.031950028815367966
0.4473471259868585 | 0.015020340116384822
0.40949994219578967 | 0.026007870458466475
0.46131237375822065 | 0.011017564823432646
0.440249556265888 | 0.017063413956873393
0.41207800232525005 | 0.02525105654091241
0.4213837321578422 | 0.022530706891882513
0.42084402300405155 | 0.02268801522371898
0.4298088028326632 | 0.020081900019140238
0.4254484274059984 | 0.021347705142543492
0.4306545823219679 | 0.019836744789891923
0.4287021045504546 | 0.02040286409441102
0.44665041888394474 | 0.015220603925594524
0.3968463088132208 | 0.02974456609704847
0.4016822051458473 | 0.028311976515202994
0.4457369277971067 | 0.015483272252149292
0.4025492255604655 | 0.02805573714633453
0.4053200212517515 | 0.02723805325110423
0.44891724149151924 | 0.014569237483431561
0.44385474365550737 | 0.016024816332868598
0.4271155287986073 | 0.02086336375309243
0.44164470897676256 | 0.016661283253930646
0.4357140748813547 | 0.018372610171739707
0.4308266042613399 | 0.0197868974011726
0.42312848670730974 | 0.022022537950321323
0.4457436351655682 | 0.01548134321073941
0.4729815059782754 | 0.007686505887107888
0.4533803067347144 | 0.013288540386716017
0.44140108602148315 | 0.01673148439952736
0.42827246450734896 | 0.02052752363756345
0.43897880587192584 | 0.017429924321158467
0.4702871347947694 | 0.008454756788032345
0.468905337198459 | 0.008848939974411765
0.4712346269464206 | 0.008184542670892594
0.4841760298917966 | 0.004498887045905824
0.4283156661287018 | 0.020514987323164632
0.4094798456250724 | 0.02601377567761157
0.4629523519861227 | 0.01054877165742254
0.4642125006562829 | 0.010188710191055887
0.4718522191970621 | 0.008008444242171581
0.4435871961048185 | 0.01610183268316116