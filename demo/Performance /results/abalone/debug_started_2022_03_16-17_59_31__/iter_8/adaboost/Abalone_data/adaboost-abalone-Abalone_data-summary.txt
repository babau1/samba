Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 98, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8065134099616859
		- Score on test : 0.7971291866028708

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8065134099616859
		- Score on test : 0.7971291866028708

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8064958896065766
		- Score on test : 0.7971110313402001

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   421 │    103 │
├──────┼───────┼────────┤
│ >=10 │   109 │    412 │
╘══════╧═══════╧════════╛



 Classification took 0:00:48

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.2814506692600572
- Feature index : 2, feature importance : 0.19382237384333115
- Feature index : 5, feature importance : 0.1597696267385756
- Feature index : 4, feature importance : 0.15922190975216582
- Feature index : 6, feature importance : 0.08875505794493356
- Feature index : 3, feature importance : 0.0578654424986113
- Feature index : 0, feature importance : 0.03720053074145151
- Feature index : 1, feature importance : 0.021914389220873735


 Estimator error | Estimator weight
0.23084291187739472 | 0.11215097148896612
0.31555766842528626 | 0.07214804525446045
0.3409686303890492 | 0.061405710525516424
0.41644098353681336 | 0.03143997892197576
0.4055059245436684 | 0.03564946237806873
0.42105137062817627 | 0.029674915512934427
0.41190769731448484 | 0.03318098823286532
0.44362900825663665 | 0.0211009518766493
0.4386095948596661 | 0.02299817231769145
0.4449796858010619 | 0.02059118631329326
0.4526876498900218 | 0.017687720410133032
0.45897259304830273 | 0.015326666652814415
0.4704326476826601 | 0.011033556457967424
0.47861595103492727 | 0.00797537343971477
0.4745575915647877 | 0.009491384395243984
0.4812381594945056 | 0.006996413730765912
0.47709678345110773 | 0.008542728266261929
0.47677191291215315 | 0.008664075990667722
0.471141195470234 | 0.010768553722212717
0.4622785225828654 | 0.01408674876817217
0.4697743110754519 | 0.011279819721544309
0.47758366466377067 | 0.008360878797788525
0.4873133838161409 | 0.004729716538275939
0.4884731785852177 | 0.004297170712867942
0.48873809656083034 | 0.004198376131929338
0.48899134106115216 | 0.004103937090043937
0.4892336717857393 | 0.0040135699484067835
0.48962689244821866 | 0.00386693869108456
0.48764697096698123 | 0.004605300090452993
0.4810002550741411 | 0.007085214826555066
0.48263532720060565 | 0.006474964430986386
0.4906215535588876 | 0.0034960523581929674
0.49107656429958857 | 0.0033263986876835724
0.49123231570419007 | 0.003268327058948277
0.49132227406814444 | 0.0032347865359362033
0.48894673024227786 | 0.0041205730442829165
0.4881154844084429 | 0.004430567716040484
0.4826024044311092 | 0.006487250630783338
0.4835888846795579 | 0.006119137493062484
0.4917136582141218 | 0.0030888633384933187
0.4920584414148691 | 0.002960317925164876
0.4921825836650414 | 0.002914034704710232
0.49225235254580446 | 0.00288802334391527
0.4901388724378268 | 0.0036760294466343995
0.488168591319341 | 0.004410761958292256
0.48188465241816325 | 0.006755117280082732
0.48600456882757764 | 0.0052179009338130305
0.4945305481001531 | 0.002038718281328985
0.4927098938481713 | 0.0027174447065628324
0.4928146576648917 | 0.0026783877488752677
0.4929164530839686 | 0.002640437663736607
0.4906864181975862 | 0.0034718668144332696
0.4888412507808667 | 0.004159907968166007
0.4832574629063142 | 0.006242804759975698
0.47517915183013404 | 0.009259122495317212
0.4497024479345943 | 0.018811105923464688
0.434519195489938 | 0.024547744566833403
0.457979664281753 | 0.0156993328500201
0.46297854782193587 | 0.013824361445056681
0.45269854175381424 | 0.01768362399936405
0.45863009145803646 | 0.015455200443095712
0.47279658003592484 | 0.010149593818159571
0.48559225754595603 | 0.005371706422373545
0.4858390020513814 | 0.005279661848819005
0.4870544116819707 | 0.004826307071062171
0.4874944614384622 | 0.004662180343904966
0.49103426775206815 | 0.0033421689897611996
0.48135623754426526 | 0.006952340656899088
0.47940102075185015 | 0.007682236497230156
0.4880094246546923 | 0.004470122073107874
0.49234205332373915 | 0.002854581102201196
0.492573057879485 | 0.002768458830731935
0.4852945499874711 | 0.005482765539068688
0.4858675059095099 | 0.005269029051493106
0.48595247948855813 | 0.005237331551202321
0.48646761718501164 | 0.005045177330195588
0.4891500858507199 | 0.004044739623064342
0.4894006607450777 | 0.003951299478084497
0.4898850847858893 | 0.003770661860895935
0.4826535425144534 | 0.006468166819709638
0.48905643679672933 | 0.0040796621613437695
0.49091376904949036 | 0.0033870972919859857
0.48949574195743284 | 0.003915843925385404
0.48709085396183643 | 0.0048127147997593725
0.4914678458715763 | 0.003180511264970463
0.49400483284952856 | 0.002234694687221104
0.4899273032269752 | 0.003754919302677875
0.4901710559901565 | 0.003664028970614733
0.49008700315456943 | 0.0036953703245275293
0.48474828591323615 | 0.005686558272310472
0.48595000360635277 | 0.0052382551195981055
0.48659886785738027 | 0.004996220557173581
0.4897708498055564 | 0.0038132584655450673
0.4945084825235164 | 0.0020469438048466688
0.49124472299844757 | 0.0032637010492251816
0.49157732214987254 | 0.0031396942748638016
0.476563134420478 | 0.008742064138671348
0.4647349943217102 | 0.013166238922737173