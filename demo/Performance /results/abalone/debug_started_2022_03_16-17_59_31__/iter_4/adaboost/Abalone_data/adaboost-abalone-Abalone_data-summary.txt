Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 90, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8275862068965517
		- Score on test : 0.7923444976076555

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8275862068965517
		- Score on test : 0.7923444976076555

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8276839890389509
		- Score on test : 0.7924114664986593

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   403 │    121 │
├──────┼───────┼────────┤
│ >=10 │    96 │    425 │
╘══════╧═══════╧════════╛



 Classification took 0:00:50

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.36183213369099115
- Feature index : 5, feature importance : 0.18785983601442932
- Feature index : 4, feature importance : 0.1442559635131054
- Feature index : 6, feature importance : 0.0880506106070626
- Feature index : 2, feature importance : 0.07567779218617215
- Feature index : 1, feature importance : 0.053648355033941275
- Feature index : 3, feature importance : 0.04928744979985445
- Feature index : 0, feature importance : 0.03938785915444393


 Estimator error | Estimator weight
0.23754789272030663 | 0.09255081239116618
0.31212176473766684 | 0.06271412610252591
0.3385404457109254 | 0.053157771518452926
0.36417968393573574 | 0.04422652264733853
0.4530496508289526 | 0.01494852592227848
0.4487133529509785 | 0.01633851593027787
0.4245527090202073 | 0.02413520587169691
0.42885514185189866 | 0.022739385030629184
0.42835222425951136 | 0.02290236031199421
0.45429492575890096 | 0.014549787392038573
0.45423991519303974 | 0.014567397939094737
0.4712599395643708 | 0.00913366037855794
0.45998590494770725 | 0.012729780893863441
0.46529784577354594 | 0.011034011175749509
0.46238092172047446 | 0.011964866231504827
0.4538335134498527 | 0.014697510547297283
0.4665197962551036 | 0.010644288481328597
0.47730886357642677 | 0.007208300143252076
0.47596213614282834 | 0.007636757429255564
0.4842772487083774 | 0.004992866457187246
0.4866513986674019 | 0.004238548649233056
0.4785654015401998 | 0.0068086311341044896
0.4666152470448212 | 0.010613851345968326
0.45897422440219443 | 0.013053064266856642
0.47132406940663996 | 0.009113234891148733
0.48645496500626473 | 0.004300952061726459
0.47689502016331026 | 0.0073399517202359385
0.4787574744576687 | 0.006747545926501576
0.4869890184173124 | 0.004131296026035491
0.477006262117719 | 0.007304562523283903
0.46387025025600526 | 0.011489489888666092
0.4685398530250027 | 0.01000030047842138
0.46946956864450845 | 0.009704021670660083
0.47633973046385614 | 0.007516615979861578
0.48686356273052916 | 0.004171149444167366
0.47856083912206243 | 0.0068100821505010815
0.4717779565548757 | 0.00896867947411488
0.4811563155147842 | 0.005984801677164567
0.4915786253182638 | 0.0026736358746441377
0.4932716638159619 | 0.002136053359425811
0.4903786368119903 | 0.0030546989748837386
0.47705046450665034 | 0.007290500700815661
0.4749556053287315 | 0.007957054471782587
0.4837216161549388 | 0.005169434011747151
0.4848411972944823 | 0.004813668882392711
0.4723528496122157 | 0.008785607011752385
0.46290173318987293 | 0.011798605342296323
0.4643359443936072 | 0.011340888353550882
0.46549625218732826 | 0.010970723529673234
0.4669364941349109 | 0.010511418501499782
0.46986758950325336 | 0.00957720241590912
0.4827237912417638 | 0.005486552774311544
0.49484392195728333 | 0.0016368658016307964
0.49489654953505213 | 0.0016201573080802062
0.4949481136386515 | 0.0016037864869983423
0.49499684272451533 | 0.0015883157724264714
0.4950464099051608 | 0.0015725790069662112
0.4931865955385988 | 0.0021630634154543617
0.49466446385374896 | 0.001693841424762965
0.4879799172302809 | 0.0038165358040076016
0.4738732765499162 | 0.008301544186936587
0.4843028464883462 | 0.004984732358231856
0.49238557560038254 | 0.0024174018713328345
0.4875303995199472 | 0.003959321787793126
0.4737544471237177 | 0.008339370589798376
0.4789483005989118 | 0.006686859210896455
0.49390679391444714 | 0.0019343968010199243
0.4951434677803956 | 0.0015417648154982372
0.49519304571768474 | 0.0015260247255554007
0.49524167773973043 | 0.0015105849755932214
0.48743292997529686 | 0.003990283106703864
0.4613542477548528 | 0.012292692897196339
0.4466942232577725 | 0.01698658338434559
0.4546548713656349 | 0.0144345667042725
0.46340480011637836 | 0.011638033657565532
0.46134116256602276 | 0.01229687179087511
0.44763242180162466 | 0.01668538677870986
0.4643901139857738 | 0.01132360430044486
0.4727663962025666 | 0.008653929172427053
0.47591406907819944 | 0.007652051839969315
0.4729120035073484 | 0.008607569026901784
0.45971407867086145 | 0.012816632745990797
0.4525566148031386 | 0.015106448371985777
0.46073223074864084 | 0.012491359534550394
0.43332128195293024 | 0.021294136152960157
0.46227942037026354 | 0.011997271985052732
0.47957182985849156 | 0.0064885773553709485
0.46347324750379665 | 0.011616188086642495
0.47510749882190395 | 0.00790871501995552
0.4808358684587657 | 0.006086675436269939