Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 94, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.834610472541507
		- Score on test : 0.7990430622009569

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8346104725415069
		- Score on test : 0.7990430622009569

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8346863378351927
		- Score on test : 0.7990359115617354

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   420 │    104 │
├──────┼───────┼────────┤
│ >=10 │   106 │    415 │
╘══════╧═══════╧════════╛



 Classification took 0:00:46

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.27474284537872806
- Feature index : 5, feature importance : 0.1662686622343232
- Feature index : 4, feature importance : 0.1570594030767147
- Feature index : 6, feature importance : 0.14898149674227482
- Feature index : 3, feature importance : 0.08378413850471197
- Feature index : 2, feature importance : 0.08101643554179279
- Feature index : 1, feature importance : 0.04525391336250641
- Feature index : 0, feature importance : 0.04289310515894799


 Estimator error | Estimator weight
0.21583652618135388 | 0.0919805126038187
0.29131531522849496 | 0.06338371111050488
0.37275316167793965 | 0.037104860117548925
0.40041789302990427 | 0.028784487196887715
0.4155687886487576 | 0.024311768476209555
0.426501793905477 | 0.021113887807032328
0.44375197768085145 | 0.016109540398323338
0.43718597379269597 | 0.018009052477282916
0.4402987535664476 | 0.01710778894582863
0.4287585918160627 | 0.020456499255881243
0.458811949187487 | 0.011773082987024447
0.4651652329255603 | 0.00995063452902789
0.4748852022261088 | 0.007168512474547235
0.46524615685529247 | 0.009927443560107819
0.46859933967487644 | 0.008966943408155601
0.4840498664475103 | 0.004550356487617099
0.4844500453792944 | 0.004436116247997667
0.471688189042899 | 0.008082880578893337
0.473895809083584 | 0.007451419397128326
0.4860895260724547 | 0.003968146790762422
0.48633503746992446 | 0.003898076204892719
0.47711847024919907 | 0.006530135947345698
0.46374868571475425 | 0.01035667116555585
0.42575372353418484 | 0.021331990599194692
0.44216193483104904 | 0.01656898448067961
0.44825096093456807 | 0.014811326351090425
0.46528411866659003 | 0.009916564743886143
0.44649671791711554 | 0.01531722314554553
0.46815619507561523 | 0.009093831190682184
0.48078160692969485 | 0.005483587764890598
0.48185832570235 | 0.005176089428059927
0.4880734913725603 | 0.0034019617768071956
0.4883535057539111 | 0.003322060179118123
0.48861690218473836 | 0.003246902388187556
0.4798452401211414 | 0.005751044562628041
0.47584515208991096 | 0.006894078463350779
0.4916967063317255 | 0.0023682308949094853
0.4874984296740957 | 0.0035660613462223387
0.4876808296450886 | 0.003514010700203451
0.4847891403782975 | 0.004339318277021934
0.48524219140612934 | 0.004209996670651346
0.4894928095633743 | 0.002996982872981867
0.48599049115288884 | 0.003996412555938832
0.472241440043922 | 0.007924601507672664
0.4702296344974441 | 0.00850025352124198
0.4730261970851429 | 0.007700124447121224
0.47761176880640954 | 0.00638916340486973
0.4675045549988074 | 0.009280444698133723
0.4532747265098607 | 0.013364558316496411
0.47359857074850326 | 0.007536423029285643
0.4913570698202604 | 0.0024651195306497464
0.48396865035398484 | 0.0045735421893092565
0.48446669019526406 | 0.004431364728462751
0.48490716997537986 | 0.00430562652117984
0.47592587881767806 | 0.0068710023626017
0.45870240479661445 | 0.011804537926183899
0.4654440801978788 | 0.009870725411149285
0.47020104871136437 | 0.008508434922480192
0.4391150804078612 | 0.017450341479443954
0.4238436921678827 | 0.02188931976987074
0.4267310695949086 | 0.021047061275658304
0.44529197934652515 | 0.015664873130527744
0.43756512967239936 | 0.01789919782865824
0.45943558276003543 | 0.011594032462741558
0.4797698118356659 | 0.005772591067979092
0.48377309680598735 | 0.004629370126575294
0.4844219273245935 | 0.004444143002926966
0.4851332550976077 | 0.004241091464573332
0.4891483060936037 | 0.003095276532696313
0.48852943952001104 | 0.003271858866989517
0.49081524581216057 | 0.0026196912564092115
0.4919257243467456 | 0.00230289980938902
0.4920685792664626 | 0.0022621486289330643
0.4608979776630257 | 0.011174308427406129
0.45715764702557155 | 0.0122482282924556
0.486392575553694 | 0.0038816547265393834
0.48356321734210206 | 0.004689289501232714
0.47997898522446814 | 0.005712840181619072
0.4794596966806601 | 0.005861179949796118
0.47735647865559494 | 0.006462117448379666
0.48575812797751433 | 0.004062733056190559
0.4918325906180972 | 0.002329467590641035
0.49202961305605397 | 0.0022732641984925
0.49221990573491187 | 0.002218981234437379
0.4720867241900249 | 0.007968862042838901
0.4500856150439617 | 0.014282637181136105
0.4402442021594689 | 0.01712357166004148
0.4556227824153547 | 0.012689311538352439
0.46837719776742986 | 0.009030548612792394
0.4839129408995298 | 0.004589446354153519
0.47699651822831457 | 0.006564988778511285
0.4534862897391391 | 0.013303693549989719
0.4650850414874568 | 0.009973616100841189
0.46108636331516917 | 0.011120253793517882