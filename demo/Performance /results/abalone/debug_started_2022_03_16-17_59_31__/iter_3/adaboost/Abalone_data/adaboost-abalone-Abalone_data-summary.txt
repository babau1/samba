Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 44, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8180076628352491
		- Score on test : 0.7971291866028708

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8180076628352491
		- Score on test : 0.7971291866028708

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8180955177138383
		- Score on test : 0.7972154254150122

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   402 │    122 │
├──────┼───────┼────────┤
│ >=10 │    90 │    431 │
╘══════╧═══════╧════════╛



 Classification took 0:00:45

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.3057531765794367
- Feature index : 6, feature importance : 0.18922337349564464
- Feature index : 5, feature importance : 0.13226747056705132
- Feature index : 2, feature importance : 0.1141564595547075
- Feature index : 3, feature importance : 0.10468477306705182
- Feature index : 4, feature importance : 0.07005508722916477
- Feature index : 0, feature importance : 0.060952774839324486
- Feature index : 1, feature importance : 0.022906884667618543


 Estimator error | Estimator weight
0.20210727969348666 | 0.14272000441550287
0.3429182099380037 | 0.06759017945840606
0.3949853647750013 | 0.04431798493827495
0.3464206499997471 | 0.06597853542266613
0.3637873190597305 | 0.058095467834234464
0.395437051432032 | 0.04412157515657642
0.43358524697347417 | 0.027775205504556064
0.4513620924668021 | 0.020284727354478622
0.4309339667067833 | 0.02889805385879852
0.46171275603067874 | 0.01594864949939556
0.45326769790093924 | 0.019485223586948597
0.45995906906309414 | 0.016682223722046335
0.46083080540405663 | 0.01631752225416663
0.4720586693588592 | 0.011628362779181596
0.47210704291856564 | 0.01160818920478111
0.4492858027457625 | 0.02115652471851281
0.461744580522152 | 0.015935340905158215
0.47359733037486323 | 0.01098678904193543
0.44903448869891877 | 0.021262096780940862
0.4617932096953985 | 0.015915005060291823
0.4454042934023052 | 0.022788317628740833
0.4504064720343654 | 0.020685885297741624
0.46801358136731736 | 0.013316126634809664
0.481966144688399 | 0.00750059647852781
0.4787684722172512 | 0.008832044944597833
0.46562465585519164 | 0.014313686693097237
0.4672021155310878 | 0.013654905027053237
0.4792089299058154 | 0.008648606392173158
0.4784645800372812 | 0.00895861570462565
0.46183822583008755 | 0.01589618039411404
0.47361951388595414 | 0.010977540766877441
0.4909928240304582 | 0.0037450224505877687
0.4759690098979734 | 0.009998277646412547
0.4772879948797525 | 0.009448725599879373
0.4794321185539974 | 0.00855565959567465
0.47719056286406514 | 0.009489315815757834
0.47934216486013703 | 0.00859312035129682
0.4765836135901719 | 0.009742187527229954
0.4747736857496939 | 0.010496427144203386
0.4711718243757194 | 0.011998249309560408
0.44846680213068757 | 0.021500611007960052
0.4555686654456114 | 0.018520607490222142
0.44414869036134447 | 0.023316764987906722
0.42289109009375114 | 0.032314863614096585