Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 43, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.859514687100894
		- Score on test : 0.8019138755980861

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.859514687100894
		- Score on test : 0.8019138755980861

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8596129379526326
		- Score on test : 0.8019479568064938

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   414 │    110 │
├──────┼───────┼────────┤
│ >=10 │    97 │    424 │
╘══════╧═══════╧════════╛



 Classification took 0:00:48

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.30482836716354145
- Feature index : 5, feature importance : 0.1796681080643958
- Feature index : 4, feature importance : 0.17032124119138026
- Feature index : 6, feature importance : 0.10490823111855388
- Feature index : 1, feature importance : 0.07734278502573404
- Feature index : 2, feature importance : 0.06473104169127016
- Feature index : 3, feature importance : 0.06153626158643855
- Feature index : 0, feature importance : 0.03666396415868606


 Estimator error | Estimator weight
0.20306513409961693 | 0.10757296550971311
0.33368483913884855 | 0.0544114059719575
0.338916684535825 | 0.05256716048899646
0.3597191309055877 | 0.045364754397065055
0.39149345436116056 | 0.03470030540135379
0.3970782469868495 | 0.03286042251998111
0.4092405468076056 | 0.028883357382835293
0.4062157873781038 | 0.02986885830912488
0.4199652653622024 | 0.025406565936086103
0.42715940838119376 | 0.023088241483946587
0.41472651887649264 | 0.027101611579832496
0.4135531727213314 | 0.02748210034066536
0.43188496207151145 | 0.02157088252171382
0.4364693874532911 | 0.020102642024390677
0.4366485171820757 | 0.020045344914175856
0.4411135655027895 | 0.018618804977256924
0.4365182433176378 | 0.02008701427828531
0.4494240719873734 | 0.015971587212087506
0.47235160148543687 | 0.008710230321880806
0.4824318700210526 | 0.0055312179331772566
0.4708884550570366 | 0.00917219199551848
0.4584203850435673 | 0.013115987960587954
0.4702269894347045 | 0.009381089016246352
0.48438691558657665 | 0.004915258264076226
0.4790966252371357 | 0.0065824267886650445
0.45194020440549143 | 0.01517194296884318
0.45570709923719227 | 0.013976243462983845
0.46011598457908987 | 0.012578793084367275
0.4407911419526418 | 0.018721711454206626
0.44246452336408865 | 0.018187797093041868
0.45847748137809763 | 0.013097893973604741
0.43721975697463566 | 0.019862661232362463
0.448511196218387 | 0.01626190663421643
0.4239023368138559 | 0.02413654652264226
0.445910154392522 | 0.01708972105645342
0.44193851752254054 | 0.018355580681772187
0.4112592988464402 | 0.02822687423573601
0.40485547495879376 | 0.030312814955238153
0.43798872651620324 | 0.019616827147327653
0.47608854659083405 | 0.0075310200356937035
0.42976444775083017 | 0.02225126279218522
0.43069461603320747 | 0.02195271219939622
0.43818134629236644 | 0.019555262940309