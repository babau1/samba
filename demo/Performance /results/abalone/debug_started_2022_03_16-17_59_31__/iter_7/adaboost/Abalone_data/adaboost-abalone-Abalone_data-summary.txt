Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 90, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.7978927203065134
		- Score on test : 0.7942583732057417

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.7978927203065133
		- Score on test : 0.7942583732057417

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.7979863965550988
		- Score on test : 0.7943583244201551

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   398 │    126 │
├──────┼───────┼────────┤
│ >=10 │    89 │    432 │
╘══════╧═══════╧════════╛



 Classification took 0:00:42

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.5090047101517862
- Feature index : 5, feature importance : 0.1692343858672227
- Feature index : 3, feature importance : 0.10470560097873825
- Feature index : 6, feature importance : 0.07152228308525876
- Feature index : 2, feature importance : 0.05021668888789028
- Feature index : 0, feature importance : 0.04757141030214896
- Feature index : 4, feature importance : 0.030383337003281285
- Feature index : 1, feature importance : 0.01736158372367345


 Estimator error | Estimator weight
0.2429757343550448 | 0.11637758915886634
0.3817636564466276 | 0.04936674998102573
0.40198070166146327 | 0.04067758799863212
0.4002693099715329 | 0.041407145046744316
0.3712107668641759 | 0.053970615990076944
0.4003439466974557 | 0.0413753063125943
0.4362658652775327 | 0.0262498131942065
0.426691802270094 | 0.030246774537090317
0.4436784137955665 | 0.02316899178173717
0.44580757478195854 | 0.02228605161103666
0.4715970544861211 | 0.011647062636685164
0.48082634731703344 | 0.007857838593953132
0.4815344513498695 | 0.007567370737918136
0.482192116044926 | 0.007297620628882541
0.48272981011829663 | 0.007077096944138839
0.4809414744877969 | 0.007810610726108315
0.48146532625386906 | 0.007595724843010522
0.4840955668620094 | 0.006517033680947889
0.48448979534101466 | 0.006355388761965337
0.48495645234681106 | 0.006164056294719892
0.48517713302857834 | 0.006073579517690731
0.4856039155189231 | 0.00589860984125985
0.4835144166708419 | 0.006755336692231788
0.4840406161747095 | 0.006539565669739459
0.4864968552616501 | 0.0055325554056140985
0.48685193569239943 | 0.005387002602864889
0.4870850798999358 | 0.005291436257494467
0.4874102705985384 | 0.00515814368241255
0.4709510591128759 | 0.011912554097397828
0.4466483678047307 | 0.021937612703471637
0.4240613322728153 | 0.0313488283910177
0.4544713842373492 | 0.01870140863415469
0.4377309992312334 | 0.025639976675437902
0.47722916382649844 | 0.009333934102717769
0.4905557187189047 | 0.0038690630147664292
0.4905571615964689 | 0.003868471766794598
0.4864957896471105 | 0.005532992225357307
0.4870168641130407 | 0.005319397824737083
0.4876730507114736 | 0.005050435809463545
0.48796964584271335 | 0.004928871245352665
0.48825230373589235 | 0.004813022327938157
0.48604823839136413 | 0.005716458207391153
0.48574136109803717 | 0.005842262587564368
0.4883338775442479 | 0.0047795894224472914
0.4891364096191036 | 0.004450686127771086
0.4923086859077159 | 0.003150794208764033
0.491714243884287 | 0.0033943540392065642
0.48875316795116724 | 0.0046077478450573
0.4890005850953043 | 0.004506349698878528
0.48936837985051695 | 0.004355621663299272
0.4899607746192849 | 0.0041128588815744676
0.4900063142964613 | 0.004094197254526207
0.4896801235588708 | 0.004227867922329834
0.4842934893001957 | 0.0064358788350439376
0.45222141441582653 | 0.019631146065597106
0.4540929964503387 | 0.01885771207378675
0.44241052980384143 | 0.023695175211369175
0.43494991515578524 | 0.026797949124549775
0.488082095125972 | 0.0048827829045111825
0.4898057331875803 | 0.004176393560653385
0.49003537056176033 | 0.004082290372405251
0.49018099099692986 | 0.004022617413455462
0.4933337183636383 | 0.0027308297510297624
0.4933945811139547 | 0.002705894524978607
0.490472078235732 | 0.003903336487952011
0.4906552760518765 | 0.003828267554925121
0.493431436590549 | 0.0026907950222566034
0.49026849248606874 | 0.003986761011554752
0.49057288020782014 | 0.0038620307552433802
0.4908347392393787 | 0.003754729957295263
0.4909944103167332 | 0.0036893032730635727
0.4912570859857003 | 0.003581671147182113
0.4913909214167795 | 0.0035268324511163224
0.49153664479682324 | 0.003467123275716329
0.49165706481254007 | 0.0034177824288837994
0.49146471879125797 | 0.003496594398719526
0.49160788208893014 | 0.003437934489221959
0.491951069176962 | 0.0032973188586031188
0.4920586447178985 | 0.003253242081455983
0.49188023248680063 | 0.00332634284389623
0.4920090671054166 | 0.0032735554101582264
0.49232254667321373 | 0.0031451151672815592
0.4924267945401908 | 0.003102402820571323
0.49226172138017565 | 0.003170036580340519
0.4877226203031335 | 0.005030118623095912
0.480820404325062 | 0.007860276571388283
0.48206533160689946 | 0.007349620990546011
0.4832078470342657 | 0.006881054150858659
0.4842598258898457 | 0.0064496818204196026
0.48523126537486483 | 0.006051386183808243