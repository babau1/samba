Classification on abalone for Abalone_data with adaboost.

Database configuration : 
	- Database name : abalone
	- View name : Abalone_data	 View shape : (4177, 8)
	- Learning Rate : 0.7498204452956667
	- Labels used : <10, >=10
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 75, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8416347381864623
		- Score on test : 0.7913875598086124

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8416347381864623
		- Score on test : 0.7913875598086123

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8417058132706987
		- Score on test : 0.7914407847504066

Test set confusion matrix : 

╒══════╤═══════╤════════╕
│      │   <10 │   >=10 │
╞══════╪═══════╪════════╡
│ <10  │   405 │    119 │
├──────┼───────┼────────┤
│ >=10 │    99 │    422 │
╘══════╧═══════╧════════╛



 Classification took 0:00:50

 Classifier Interpretation : 
Feature importances : 
- Feature index : 7, feature importance : 0.2650932418516146
- Feature index : 5, feature importance : 0.1933606920810708
- Feature index : 6, feature importance : 0.1271362777180367
- Feature index : 3, feature importance : 0.10985590555228392
- Feature index : 4, feature importance : 0.10818413231394175
- Feature index : 2, feature importance : 0.104926894483958
- Feature index : 1, feature importance : 0.05093406271181906
- Feature index : 0, feature importance : 0.040508793287275024


 Estimator error | Estimator weight
0.20593869731800774 | 0.10039621529297037
0.3448957836564087 | 0.04772543672995589
0.3455337464869244 | 0.04751548219876931
0.3913933691677552 | 0.03284039840205366
0.4030558379924378 | 0.029216791738072113
0.40903904768110577 | 0.027371224513054064
0.4227877343563395 | 0.023160754994995328
0.38423629534967774 | 0.03508301879020106
0.40145707898801064 | 0.02971142526992787
0.4595360218028436 | 0.012066947056897582
0.44952911492297015 | 0.015069558635955863
0.4700581530191613 | 0.008920240104716813
0.47426081476387527 | 0.007665789352102922
0.4459920995937589 | 0.01613366149810806
0.42248951434093773 | 0.023251670491481646
0.43928516694705816 | 0.01815604139871539
0.4315903028195349 | 0.020484617190868793
0.45889624817872987 | 0.012258594730374455
0.4867292858327993 | 0.00394979285594346
0.48730272625619875 | 0.003779043401061073
0.48769763283796436 | 0.003661460567926672
0.48785752034242963 | 0.00361385561580689
0.4880958049147691 | 0.003542910067771543
0.4884812907173003 | 0.003428141025637166
0.4911255037269798 | 0.002640993527263592
0.48982620691651824 | 0.0030277559200729155
0.48766677085171956 | 0.003670649510697891
0.48585625524013676 | 0.004209769363638635
0.44948211457740617 | 0.015083688274751953
0.46609560547500073 | 0.010104177590606986
0.4747393519887662 | 0.007523023336586556
0.4578086681048959 | 0.012584478777424364
0.46436627259356056 | 0.010621262905253755
0.46268612600931913 | 0.011123886431022564
0.46607792941076476 | 0.010109461631093752
0.4681214750164307 | 0.009498734207285723
0.4832547089143199 | 0.004984632740697989
0.4707811618021873 | 0.008704344224205149
0.4635926563625298 | 0.010852662981331465
0.44749662714786864 | 0.015680833171017608
0.43125837820282875 | 0.02058527896846162
0.42920047207470263 | 0.021209795776575495
0.4421440848427206 | 0.017293201838038057
0.451665530981192 | 0.01442757320580177
0.4487677494353098 | 0.015298480260967749
0.44647593453801954 | 0.015988006181289312
0.44794344558884064 | 0.015546407155949536
0.42271164540452 | 0.023183949876216643
0.46041681346053714 | 0.01180316634281425
0.4686363236950334 | 0.009344919404595267
0.483562177693325 | 0.004893040744400828
0.47987359347476854 | 0.005992099009193281
0.48328000941624977 | 0.004977095822503455
0.4760948944680936 | 0.007118687258769608
0.48132292139817034 | 0.005560182907246275
0.49037192470777535 | 0.002865307124156545
0.4846068003433456 | 0.004581884968056858
0.4851051113653572 | 0.004433470194185027
0.4871016532588359 | 0.0038389143838693162
0.4902373079743194 | 0.0029053790662661684
0.49427536643219844 | 0.0017035099906916437
0.48816666380795587 | 0.003521813239846167
0.4641788266011817 | 0.010677326027600399
0.47513230440412174 | 0.007405800914579844
0.4948799662451301 | 0.0015235826526226015
0.4932170943245322 | 0.002018461339212734
0.49016437713228106 | 0.0029270888793830386
0.4859902790108061 | 0.004169857297062874
0.43889625354365164 | 0.018273510616200143
0.4492996590319465 | 0.015138542165790919
0.4423775424974692 | 0.017222794726031002
0.44675415406646 | 0.015904263868962758
0.4714361070086508 | 0.00850880426820028
0.474411695734302 | 0.007620774259291042
0.4928974199607413 | 0.0021136027488489664