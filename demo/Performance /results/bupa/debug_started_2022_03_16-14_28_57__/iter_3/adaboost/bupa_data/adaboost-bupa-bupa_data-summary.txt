Classification on bupa for bupa_data with adaboost.

Database configuration : 
	- Database name : bupa
	- View name : bupa_data	 View shape : (345, 6)
	- Learning Rate : 0.7478260869565218
	- Labels used : 1, 2
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 54, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.7126436781609196

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.7126436781609196

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.6937837837837837

Test set confusion matrix : 

╒════╤═════╤═════╕
│    │   1 │   2 │
╞════╪═════╪═════╡
│  1 │  21 │  16 │
├────┼─────┼─────┤
│  2 │   9 │  41 │
╘════╧═════╧═════╛



 Classification took 0:00:25

 Classifier Interpretation : 
Feature importances : 
- Feature index : 1, feature importance : 0.23308136093046353
- Feature index : 2, feature importance : 0.20442971955424558
- Feature index : 3, feature importance : 0.173913852423907
- Feature index : 4, feature importance : 0.16981328600544793
- Feature index : 0, feature importance : 0.13031854539866256
- Feature index : 5, feature importance : 0.08844323568727332


 Estimator error | Estimator weight
0.22868217054263568 | 0.037066975410972286
0.29013712630951366 | 0.027278652463171328
0.2779160235048379 | 0.0291111447694155
0.29461782770722267 | 0.026618349049023122
0.32736055342834236 | 0.021956240683603117
0.3181478012842019 | 0.02324132012849974
0.34391417244073547 | 0.019692533950902865
0.35814468620219686 | 0.017787803928306537
0.3470270789455842 | 0.019272809105690056
0.3829451388014783 | 0.014545053027682312
0.3889454408288095 | 0.013773112810739108
0.39725598348090546 | 0.01271103075281148
0.4212223201922938 | 0.009687967920115026
0.4148235979610239 | 0.010489887278767405
0.33314875494510165 | 0.0211583746199279
0.3758675335885663 | 0.015461528154103852
0.2789687513175996 | 0.028951392589571006
0.33682483980231714 | 0.020655259346934483
0.3307919557047126 | 0.02148238996758427
0.3878617908075487 | 0.013912196995655617
0.30547804491847064 | 0.0250416382641907
0.32664295400426613 | 0.022055656357512922
0.34163440241977305 | 0.020001070146445265
0.34717097586710544 | 0.019253449903389
0.28406348208260357 | 0.028183418378531062
0.35288426437728315 | 0.018487795389110752
0.3554052573907297 | 0.018151753299584503
0.34117718863299523 | 0.020063066655460305
0.3990999256829833 | 0.012476424671381325
0.3771182359253695 | 0.015299088243735518
0.4125429181187684 | 0.010776570065824177
0.3960936942669514 | 0.012859100051586128
0.3677144221115054 | 0.0165258432641865
0.3049898852997172 | 0.025111820491439098
0.37217176842050265 | 0.015942797936316075
0.3746262377568179 | 0.01562295907889199
0.39694803812656193 | 0.012750246854034714
0.4293038797761094 | 0.008679840062911235
0.3794522459516695 | 0.014996515464025513
0.3389785429041709 | 0.020361757556282865
0.3893987368586701 | 0.013714975082549326
0.41578668360291554 | 0.010368965328410178
0.37834098886638806 | 0.01514048374549503
0.3979945834968514 | 0.012617013978674537
0.32142101541056245 | 0.022782534231444437
0.3414049082206071 | 0.020032183718084056
0.3703624767572121 | 0.016179113994367136
0.3621126226964965 | 0.01726280991906749
0.39205390095568005 | 0.013374923944535906
0.3916038815943309 | 0.01343250049719298
0.3645385884175055 | 0.01694306095130701
0.33385044884235004 | 0.02106212727686911
0.3274670447173825 | 0.021941497006266394
0.2876150143852788 | 0.027652975237419757