Classification on bupa for bupa_data with adaboost.

Database configuration : 
	- Database name : bupa
	- View name : bupa_data	 View shape : (345, 6)
	- Learning Rate : 0.7478260869565218
	- Labels used : 1, 2
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 83, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8488372093023255
		- Score on test : 0.7126436781609196

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8488372093023255
		- Score on test : 0.7126436781609196

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8362962962962963
		- Score on test : 0.7043243243243243

Test set confusion matrix : 

╒════╤═════╤═════╕
│    │   1 │   2 │
╞════╪═════╪═════╡
│  1 │  24 │  13 │
├────┼─────┼─────┤
│  2 │  12 │  38 │
╘════╧═════╧═════╛



 Classification took 0:00:16

 Classifier Interpretation : 
Feature importances : 
- Feature index : 4, feature importance : 0.335519444837902
- Feature index : 2, feature importance : 0.1810292313249775
- Feature index : 1, feature importance : 0.14386569427358714
- Feature index : 3, feature importance : 0.1280096184621605
- Feature index : 5, feature importance : 0.12369521142152404
- Feature index : 0, feature importance : 0.08788079967984881


 Estimator error | Estimator weight
0.32945736434108525 | 0.05128732726481353
0.4005780346820809 | 0.029088873626873245
0.3735656443694335 | 0.03730860043245716
0.3984074441459856 | 0.029741871540811083
0.40273385046324217 | 0.028441481383025942
0.3962812015842979 | 0.030382693754814904
0.4196139438592007 | 0.02340913391956199
0.41326587928820635 | 0.025294389831857386
0.411257858137554 | 0.025892487539748522
0.42129775616868914 | 0.022910424661240388
0.41694015181068955 | 0.024202198922831535
0.4087189837245234 | 0.026649966192014936
0.42406045190827774 | 0.022093341129568407
0.44197475323824686 | 0.01682667566370307
0.45080633595337294 | 0.014247432899527995
0.4751362390869694 | 0.007183653669368359
0.4605794828973357 | 0.011403674113210657
0.438164032100582 | 0.017942804338890627
0.455133015209451 | 0.012987239460672761
0.42790656093314383 | 0.020958155113155113
0.45210667867154203 | 0.013868474695638042
0.4535047645317052 | 0.013461243633308606
0.452273565474418 | 0.013819852867887311
0.45858674562323354 | 0.011982726526159644
0.4688122759385424 | 0.009015048632206512
0.46921673341764153 | 0.008897838764778418
0.4710020374412255 | 0.008380603989605766
0.47235312630887394 | 0.007989312753439587
0.48619262153894877 | 0.003986959852748275
0.48540523411085296 | 0.004214448248561956
0.485819166015098 | 0.004094854386923036
0.48621026587572264 | 0.0039818623622324065
0.4741795954947835 | 0.007460531122767487
0.4751990688070806 | 0.007165470990243257
0.4463958162009247 | 0.015534279339839831
0.4166910689111663 | 0.024276151662643516
0.46246123917167353 | 0.010857207088864361
0.44362144040532864 | 0.01634500145263193
0.4466264773967636 | 0.015466920658758734
0.4590111776807352 | 0.01185936283086549
0.47164990691520275 | 0.008192958364438323
0.4553681517402758 | 0.012918811587544427
0.4650886537965015 | 0.010094714783825731
0.4775892384716651 | 0.006473928502617029
0.4774268789069491 | 0.006520893877355052
0.4744826696671937 | 0.007372807761141716
0.4751451269980129 | 0.007181081528308308
0.4785509107314644 | 0.006195775547801928
0.4559667824071242 | 0.01274462812774197
0.4601776791730818 | 0.011520400731570533
0.4686416672153475 | 0.00906449378644156
0.47147514357981246 | 0.008243573178710759
0.48071150308068267 | 0.005571012908708807
0.4902213760878106 | 0.0028232759330980048
0.4858594395826953 | 0.004083218786626433
0.4767576036311646 | 0.006714508261521175
0.4811636323791274 | 0.005440301169896716
0.48125393762981045 | 0.005414194765880189
0.4814197184229124 | 0.005366270008248305
0.48199395871292944 | 0.005200274770182472
0.4786206262959371 | 0.0061756129122681035
0.4630349817714001 | 0.010690653209822331
0.4530547817522994 | 0.013592289900468223
0.4728863159711221 | 0.00783492734582914
0.4879959667835466 | 0.00346601831193264
0.48072401391259906 | 0.005567395873267018
0.47344297015064857 | 0.0076737668729766695
0.49165343090685004 | 0.00240973086828262
0.49102423662375877 | 0.002591422683416103
0.4556610185567443 | 0.012833591524164611
0.46392739789257953 | 0.01043164670513942
0.4532359216249793 | 0.013539534753494198
0.48006191536603365 | 0.00575882642403692
0.4819206506598889 | 0.005221465102937371
0.4825515634206043 | 0.005039101528790155
0.47957128046141695 | 0.00590069523551008
0.4805679399138854 | 0.005612519288336689
0.4718000108103686 | 0.008149487004229319
0.4746241820553611 | 0.007331849548447225
0.45939243156758514 | 0.011748563868435325
0.48134729697209905 | 0.005387205824718632
0.4912729095100252 | 0.0025196127089446773
0.4914226206416789 | 0.0024763808046414644