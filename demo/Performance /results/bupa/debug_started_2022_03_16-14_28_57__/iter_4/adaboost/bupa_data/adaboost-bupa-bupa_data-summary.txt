Classification on bupa for bupa_data with adaboost.

Database configuration : 
	- Database name : bupa
	- View name : bupa_data	 View shape : (345, 6)
	- Learning Rate : 0.7478260869565218
	- Labels used : 1, 2
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 82, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5517241379310345

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5517241379310345

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 1.0
		- Score on test : 0.5291891891891892

Test set confusion matrix : 

╒════╤═════╤═════╕
│    │   1 │   2 │
╞════╪═════╪═════╡
│  1 │  14 │  23 │
├────┼─────┼─────┤
│  2 │  16 │  34 │
╘════╧═════╧═════╛



 Classification took 0:00:21

 Classifier Interpretation : 
Feature importances : 
- Feature index : 1, feature importance : 0.22540566924531952
- Feature index : 2, feature importance : 0.1910809169438966
- Feature index : 4, feature importance : 0.1867371964303658
- Feature index : 3, feature importance : 0.16584635235627085
- Feature index : 0, feature importance : 0.1374121550747706
- Feature index : 5, feature importance : 0.09351770994937648


 Estimator error | Estimator weight
0.15503875968992248 | 0.010952813976790255
0.13944954128440365 | 0.011755430421083303
0.15592385815284482 | 0.010909272471903
0.1692932500997766 | 0.010274753550545116
0.17269922774265573 | 0.010119547356913682
0.17904251294100196 | 0.009836823439565115
0.16062657574098982 | 0.010681242860139965
0.2080588922901917 | 0.008634181224881203
0.16975766659144967 | 0.010253445441241809
0.10899296906638573 | 0.013571836187396545
0.104635189738516 | 0.013866921148545342
0.17135053892020338 | 0.010180712411202027
0.16885198268243085 | 0.01029504270873975
0.11224749425607472 | 0.013358142323893384
0.13783109121519543 | 0.011842974705363296
0.14121870689252583 | 0.01166070233030406
0.11204542535288223 | 0.013371251367115838
0.1838404502631975 | 0.009628140232348219
0.1257785771525142 | 0.012523730194035654
0.10653361591074692 | 0.013737064745519162
0.17947420518885945 | 0.009817870051924018
0.11122332794304958 | 0.013424798181234985
0.10848540737238299 | 0.013605665794223952
0.10157397586293186 | 0.014080767017513688
0.13145634684912558 | 0.012196443041203844
0.1321891071627805 | 0.012155084756768304
0.13712701804862254 | 0.011881328711515975
0.12509674291851267 | 0.012563877770205524
0.14676815361130602 | 0.011369848720901105
0.07745193759936508 | 0.016003272287305925
0.1457770034333493 | 0.011421117980616713
0.15250267543507393 | 0.011078709032430868
0.13018530871795966 | 0.012268649103432679
0.14606120268770723 | 0.011406387727884474
0.13765449988166764 | 0.011852578923576232
0.12021809236390636 | 0.012856754839938965
0.13445370290303993 | 0.012028482886174627
0.13298726172318576 | 0.01211025604182531
0.14997092148118782 | 0.011206113437036692
0.10963099071144339 | 0.013529506852252772
0.1511401173643682 | 0.011147058560283831
0.0993619817155824 | 0.014238874997537692
0.14299837628379206 | 0.01156640703991477
0.10413583598474664 | 0.013901423288182052
0.13450247330843673 | 0.012025776279911167
0.1133904641347032 | 0.013284378744090372
0.12427490977550637 | 0.0126125187471355
0.2579400593677734 | 0.0068257623108247565
0.16723095235460642 | 0.010369941382090403
0.09064793186174915 | 0.014893966563097522
0.14293966018056695 | 0.011569502442062563
0.13063413312897304 | 0.01224308382612905
0.10925784753850423 | 0.01355423659116276
0.12995698744005274 | 0.012281683174401925
0.20828059475500746 | 0.008625493237911517
0.12763542778830209 | 0.012415332138841691
0.11630608442539137 | 0.013099107606509971
0.07710585365301238 | 0.01603462311921159
0.07835411144447912 | 0.015922145906833243
0.08846828558464535 | 0.015066648422786873
0.13543046467222555 | 0.011974432847595553
0.16944202448654683 | 0.010267922504367295
0.10367795370262584 | 0.013933188757273679
0.09268821968055684 | 0.014735680254039573
0.19641027092914948 | 0.009100668276795214
0.11041585917425419 | 0.01347773030438128
0.11142126326341076 | 0.013411874231263499
0.10178200161247547 | 0.014066055531285492
0.09390357137994483 | 0.01464287389521895
0.14171911871992515 | 0.011634088387401218
0.1264844389543963 | 0.012482363643195939
0.11377732652538339 | 0.013259558800297045
0.09269228101655688 | 0.01473536830935205
0.14477730760312746 | 0.011473122970828678
0.1234467822381326 | 0.012661812266087135
0.18696325096399477 | 0.009494574960348792
0.1165524048997035 | 0.013083640981771108
0.21118771721867574 | 0.008512194556304755
0.11725648199755886 | 0.013039587426059799
0.13614853993717907 | 0.011934906797466754
0.14005799145198095 | 0.011722738712959774
0.0976131519172759 | 0.014366108951293942