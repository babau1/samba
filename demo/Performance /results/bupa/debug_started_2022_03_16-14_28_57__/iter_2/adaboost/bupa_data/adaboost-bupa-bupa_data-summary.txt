Classification on bupa for bupa_data with adaboost.

Database configuration : 
	- Database name : bupa
	- View name : bupa_data	 View shape : (345, 6)
	- Learning Rate : 0.7478260869565218
	- Labels used : 1, 2
	- Number of cross validation folds : 7

Classifier configuration : 
	- Adaboost with n_estimators : 83, base_estimator : DecisionTreeClassifier
	- Executed on 6 core(s) 


	For Accuracy score using {}, (higher is better) : 
		- Score on train : 0.8527131782945736
		- Score on test : 0.7241379310344828

	For F1 score using average: micro, {} (higher is better) : 
		- Score on train : 0.8527131782945736
		- Score on test : 0.7241379310344829

	For Balanced accuracy score using {}, (higher is better) : 
		- Score on train : 0.8461111111111111
		- Score on test : 0.7002702702702703

Test set confusion matrix : 

╒════╤═════╤═════╕
│    │   1 │   2 │
╞════╪═════╪═════╡
│  1 │  20 │  17 │
├────┼─────┼─────┤
│  2 │   7 │  43 │
╘════╧═════╧═════╛



 Classification took 0:00:23

 Classifier Interpretation : 
Feature importances : 
- Feature index : 4, feature importance : 0.30149607666816064
- Feature index : 1, feature importance : 0.19636276819422496
- Feature index : 5, feature importance : 0.18537601797712344
- Feature index : 2, feature importance : 0.16021582292405537
- Feature index : 3, feature importance : 0.10509892628136003
- Feature index : 0, feature importance : 0.05145038795507548


 Estimator error | Estimator weight
0.3992248062015504 | 0.029285169881210237
0.37115565299091763 | 0.03778103878020471
0.35083446166059395 | 0.04409464424567307
0.34880560999922927 | 0.044733819821393205
0.44105528552404755 | 0.01697368874292857
0.3917053859818108 | 0.03153895322545995
0.4185591546562672 | 0.023552393545843528
0.4015479283612717 | 0.028591792869138426
0.3970696372673358 | 0.029929629048433437
0.4235605727289325 | 0.02208222552471473
0.38645138937818796 | 0.033122823061549574
0.41707880513340956 | 0.023988473021580237
0.3956044982880169 | 0.030368429664414026
0.40698594172251157 | 0.026973814589435104
0.42618982824342483 | 0.02131121970134906
0.4542415981723945 | 0.013152095037243211
0.423432375374909 | 0.02211985035254117
0.44363408232104595 | 0.016224590609026233
0.4307941076428955 | 0.019963972579746758
0.47674775538906866 | 0.006669380111295309
0.47643994448843313 | 0.006757798781324545
0.4602642558817833 | 0.011413145952634927
0.46638377322058505 | 0.009649656003292595
0.48363376263469116 | 0.004692575144622397
0.477182800638868 | 0.006544422015877492
0.4547061324756404 | 0.013017836914406056
0.4628940051357844 | 0.010654927157818326
0.46983229922760583 | 0.008657196120965622
0.4706441259121551 | 0.008423682565432297
0.4853494786618882 | 0.00420034176507
0.48492479586298637 | 0.00432217221066026
0.48536601644193744 | 0.004195597630170598
0.47956592616344496 | 0.005860087970075325
0.4677366770075456 | 0.009260195528933767
0.47387630790115653 | 0.007494409458480324
0.4867283439781444 | 0.003804822112279022
0.48568712146769144 | 0.0041034851682603905
0.48608543638538354 | 0.003989228912256453
0.4864621820436956 | 0.0038811644457012323
0.44339736813516833 | 0.01629331479405323
0.42902086562063185 | 0.02048240886962011
0.42767493742646756 | 0.0208762680374247
0.44685965640299197 | 0.01528885058290225
0.4720127337976935 | 0.008030117134994035
0.48572614391014235 | 0.004092291389597753
0.48612231992659993 | 0.003978649162167065
0.4860390302967204 | 0.004002540191021238
0.4864182588501422 | 0.0038937629982790513
0.4864765839771198 | 0.003877033531555816
0.48683271726183786 | 0.003774885665668327
0.4867585670389773 | 0.003796153470582661
0.44506369299076043 | 0.01580969481355102
0.45088555831004484 | 0.014122745244549748
0.4630988635260806 | 0.010595887182121006
0.48139292101889614 | 0.0053356338662352235
0.44605339360282276 | 0.015522622875231979
0.4703793847338418 | 0.008499827660027423
0.44027071509836324 | 0.017201774999308205
0.4233572480255661 | 0.022141900993262464
0.47877319146654407 | 0.0060876970603439585
0.487306387409418 | 0.003639031234112446
0.4811879113942775 | 0.005394476214256853
0.44264600751484096 | 0.016511503601325368
0.44488137028412356 | 0.015862592942234438
0.4729015867840887 | 0.007774578183970754
0.4876700481763814 | 0.0035347332977029296
0.4879667860724025 | 0.003449631624496491
0.48772948135664673 | 0.003517688215784223
0.4880233995547418 | 0.0034333956815893294
0.48823574587147334 | 0.0033724985546553575
0.4885061783490918 | 0.0032949451153064355
0.4882909004911269 | 0.003356681395994222
0.4885588320394232 | 0.003279845546851133
0.48875240265911035 | 0.0032243356673474135
0.48899985310504057 | 0.003153376178686637
0.4888036939665956 | 0.0032096271191762483
0.489048917312919 | 0.0031393065924676796
0.4833266211714152 | 0.004780704383553761
0.4742237836715924 | 0.0073945467930785465
0.4799415883047734 | 0.005752238668494244
0.48980963114058906 | 0.0029211728925976075
0.4892362038904682 | 0.0030856010994957457
0.46574816210760944 | 0.009832678202882602